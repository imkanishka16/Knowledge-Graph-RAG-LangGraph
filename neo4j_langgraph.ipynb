{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-29 09:23:03 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import chainlit as cl\n",
    "\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "#getting chat history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "import logging\n",
    "\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains import RetrievalQA, GraphCypherQAChain\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME_MODEL = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME_MODEL')\n",
    "NEO4J_URI=os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME=os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD=os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE=os.getenv(\"NEO4J_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME_MODEL,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\",\n",
    "                                    azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                                    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "                                    api_key=OPENAI_API_KEY\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "        embedding=embeddings,\n",
    "        search_type=\"hybrid\",\n",
    "        node_label=\"Document\",\n",
    "        text_node_properties=[\"text\"],\n",
    "        embedding_node_property=\"embedding\",\n",
    "        index_name='neo4j'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQuery(BaseModel):\n",
    "    \"\"\"Decompose a given question/query into sub-queries\"\"\"\n",
    "\n",
    "    sub_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique paraphrasing of the original questions.\",\n",
    "    )\n",
    "    \n",
    "system = \"\"\"You are an expert at converting user questions into Neo4j Cypher queries. \\\n",
    "\n",
    "Perform query decomposition. Given a user question, break it down into two distinct subqueries that \\\n",
    "you need to answer in order to answer the original question.\n",
    "\n",
    "For the given input question, create a query for similarity search and create a query to perform neo4j graph query.\n",
    "Here is example:\n",
    "Question: Find the articles about the photosynthesis and return their titles.\n",
    "Answers:\n",
    "sub_query1 : Find articles related to photosynthesis.\n",
    "sub_query2 : Return titles of the articles\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([SubQuery])\n",
    "parser = PydanticToolsParser(tools=[SubQuery])\n",
    "query_analyzer = prompt | llm_with_tools | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVE = \"retrieve\"\n",
    "GRADE_DOCUMENTS = \"grade_documents\"\n",
    "GENERATE = \"generate\"\n",
    "VECTOR_SEARCH = \"vector_search\"\n",
    "GRAPH_QA = \"graph_qa\"\n",
    "GRAPH_QA_WITH_CONTEXT = \"graph_qa_with_context\"\n",
    "PROMPT_TEMPLATE = \"prompt_template\"\n",
    "PROMPT_TEMPLATE_WITH_CONTEXT = \"prompt_template_with_context\"\n",
    "CREATE_CONTEXT = \"create_context\"\n",
    "CREATE_PREFIX = \"create_prefix\"\n",
    "DECOMPOSER = \"decomposer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        documents: result of chain\n",
    "        article_ids: list of article id from vector search\n",
    "        prompt: prompt template object\n",
    "        prompt_with_context: prompt template with context from vector search\n",
    "        subqueries: decomposed queries\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    documents: dict\n",
    "    article_ids: List[str]\n",
    "    prompt: object\n",
    "    prompt_with_context: object\n",
    "    subqueries: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_qa_chain(state: GraphState):\n",
    "    \n",
    "    \"\"\"Create a Neo4j Graph Cypher QA Chain\"\"\"\n",
    "    \n",
    "    prompt = state[\"prompt\"]\n",
    "    \n",
    "    graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "            cypher_llm = llm, #should use gpt-4 for production\n",
    "            qa_llm = llm,\n",
    "            validate_cypher= True,\n",
    "            graph=graph,\n",
    "            verbose=True,\n",
    "            cypher_prompt = prompt,\n",
    "            # return_intermediate_steps = True,\n",
    "            return_direct = True,\n",
    "        )\n",
    "    return graph_qa_chain\n",
    "\n",
    "def get_graph_qa_chain_with_context(state: GraphState):\n",
    "    \n",
    "    \"\"\"Create a Neo4j Graph Cypher QA Chain. Using this as GraphState so it can access state['prompt']\"\"\"\n",
    "    \n",
    "    prompt_with_context = state[\"prompt_with_context\"] \n",
    "    \n",
    "    graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "            cypher_llm = llm, #should use gpt-4 for production\n",
    "            qa_llm = llm,\n",
    "            validate_cypher= True,\n",
    "            graph=graph,\n",
    "            verbose=False,\n",
    "            cypher_prompt = prompt_with_context,\n",
    "            # return_intermediate_steps = True,\n",
    "            return_direct = True,\n",
    "        )\n",
    "    return graph_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vector search\", \"graph query\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to vectorstore or graphdb.\",\n",
    "    )\n",
    "    \n",
    "llm = ChatOpenAI(temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user question to perform vector search or graph query. \n",
    "The vector store contains documents related article title, abstracts and topics. Here are three routing situations:\n",
    "If the user question is about similarity search, perform vector search. The user query may include term like similar, related, relvant, identitical, closest etc to suggest vector search. For all else, use graph query.\n",
    "\n",
    "Example questions of Vector Search Case: \n",
    "    Find articles about photosynthesis\n",
    "    Find similar articles that is about oxidative stress\n",
    "    \n",
    "Example questions of Graph DB Query: \n",
    "    MATCH (n:Article) RETURN COUNT(n)\n",
    "    MATCH (n:Article) RETURN n.title\n",
    "\n",
    "Example questions of Graph QA Chain: \n",
    "    Find articles published in a specific year and return it's title, authors\n",
    "    Find authors from the institutions who are located in a specific country, e.g Japan\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neo4j_vector_index():   \n",
    "\n",
    "    ''' Create vector for article title and abstract and Instantiate Neo4j vector from graph'''\n",
    "    \n",
    "    neo4j_vector_index = Neo4jVector.from_existing_graph(\n",
    "        embedding = EMBEDDING_MODEL,\n",
    "        url = AURA_CONNECTION_URI,\n",
    "        username = AURA_USERNAME,\n",
    "        password = AURA_PASSWORD,\n",
    "        index_name = 'title_abstract_vector',\n",
    "        node_label = 'Article',\n",
    "        text_node_properties = ['title', 'abstract'],\n",
    "        embedding_node_property = 'embedding_vectors',\n",
    "    )\n",
    "    return neo4j_vector_index\n",
    "\n",
    "def get_neo4j_title_vector_index(): \n",
    "    \n",
    "    '''Create a title vector and Instantiate Neo4j vector from graph'''\n",
    "    \n",
    "    neo4j_title_vector_index = Neo4jVector.from_existing_graph(\n",
    "        embedding = EMBEDDING_MODEL,\n",
    "        url = AURA_CONNECTION_URI,\n",
    "        username = AURA_USERNAME,\n",
    "        password = AURA_PASSWORD,\n",
    "        index_name = 'title_vector',\n",
    "        node_label = 'Title',\n",
    "        text_node_properties = ['text'],\n",
    "        embedding_node_property = 'embedding_vectors',\n",
    "    )\n",
    "    return neo4j_title_vector_index\n",
    "\n",
    "def get_neo4j_abstract_vector_index(): \n",
    "    \n",
    "    ''' Create an abstract vector and Instantiate Neo4j vector from graph'''\n",
    "    \n",
    "    neo4j_abstract_vector_index = Neo4jVector.from_existing_graph(\n",
    "        embedding = EMBEDDING_MODEL,\n",
    "        url = AURA_CONNECTION_URI,\n",
    "        username = AURA_USERNAME,\n",
    "        password = AURA_PASSWORD,\n",
    "        index_name = 'abstract_vector',\n",
    "        node_label = 'Abstract',\n",
    "        text_node_properties = ['text'],\n",
    "        embedding_node_property = 'embedding_vectors',\n",
    "    )\n",
    "    return neo4j_abstract_vector_index\n",
    "\n",
    "def get_neo4j_topic_vector_index(): \n",
    "    \n",
    "    '''Create a topic vector and Instantiate Neo4j vector from graph'''\n",
    "    \n",
    "    neo4j_topic_vector_index = Neo4jVector.from_existing_graph(\n",
    "        embedding = EMBEDDING_MODEL,\n",
    "        url = AURA_CONNECTION_URI,\n",
    "        username = AURA_USERNAME,\n",
    "        password = AURA_PASSWORD,\n",
    "        index_name = 'topic_vector',\n",
    "        node_label = 'Topic',\n",
    "        text_node_properties = ['text'],\n",
    "        embedding_node_property = 'embedding_vectors',\n",
    "    )\n",
    "    return neo4j_topic_vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = get_neo4j_vector_index()\n",
    "\n",
    "def get_vector_graph_chain():\n",
    "    '''Create a Neo4j Retrieval QA Chain. Returns top K most relevant articles'''\n",
    "    vector_graph_chain = RetrievalQA.from_chain_type(\n",
    "        llm, \n",
    "        chain_type=\"stuff\", \n",
    "        retriever = vector_index.as_retriever(search_kwargs={'k':3}), \n",
    "        verbose=True,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return vector_graph_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
